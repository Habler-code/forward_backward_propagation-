{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(dim_size, input_length):\n",
    "    # Returns the weight matrix of the current layer (of shape [size of current layer, size of previous layer])\n",
    "    # randn : (mean 0 and variance 1)\n",
    "    '''\n",
    "    |initalize weights for layer L \n",
    "    |dim_size : number of neurons contains in layer L\n",
    "    |input_length : number of inputs for each neuron in layer L\n",
    "    '''\n",
    "    return np.random.randn(dim_size, input_length) / 10\n",
    "\n",
    "def initalize_bias(dim_size):\n",
    "    #zeros :  Return a new array of given shape and type, filled with zeros.\n",
    "    '''\n",
    "    |initalize bias for layer L \n",
    "    |dim_size : number of neurons contains in layer L\n",
    "    '''\n",
    "    return np.zeros((dim_size,1))\n",
    "\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    params = {}\n",
    "    inputs = layer_dims[0]\n",
    "    network_dims = layer_dims[1:]\n",
    "    prev_dim = inputs\n",
    "    layer = 0\n",
    "    for dim in network_dims:\n",
    "        params['W{}'.format(layer+1)] = initalize_weights(dim, prev_dim)\n",
    "        params['b{}'.format(layer+1)] = initalize_bias(dim)\n",
    "        layer += 1\n",
    "        prev_dim = dim\n",
    "    return params\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, B, dropout=False, training=False):\n",
    "    '''\n",
    "    Implement the linear part of a layer's forward propagation\n",
    "    |A – the activations of the previous layer\n",
    "    |W – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])\n",
    "    |B – the bias vector of the current layer (of shape [size of current layer, 1])\n",
    "    |dropout : should use dropout\n",
    "    |training : boolean - is in training mode\n",
    "    '''\n",
    "    # copy activations\n",
    "    activations = np.copy(A)\n",
    "    # ------------ DROPOUT BONUS ------------ #\n",
    "    if training and dropout:\n",
    "        # ------------ DROPOUT BONUS ------------ #\n",
    "        proba = 0.75\n",
    "        keeps = np.random.rand(activations.shape[0], activations.shape[1]) < proba\n",
    "        activations = np.multiply(activations, keeps)\n",
    "        activations /= proba\n",
    "        # ------------ DROPOUT BONUS ------------ #\n",
    "    \n",
    "        # update cache for backpropagation\n",
    "    linear_cache = {'A': activations, 'W': W, 'b': B}\n",
    "    # calculate the linear component W\n",
    "\n",
    "    # Z[L] : w[L]X + b[L] --> (nL,m) = (nL, nL-1)(nL-1,m) + (nL, m) ; m = #samples\n",
    "    # |   # -Weights-  #inputs#      # wX #\n",
    "    # n1  ( 0 1 2 3 )    (1)\n",
    "    # n2  ( 0 1 2 3 ) *  (2)   ==   (20)\n",
    "    # n3  ( 0 1 2 3 )    (3)        (20)\n",
    "    #                   (4)        (20)\n",
    "\n",
    "    # before applying non linear function\n",
    "    # Z is the input for the activation function\n",
    "    Z = np.dot(W, activations) + B\n",
    "\n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    '''\n",
    "    |Z – the linear component of the activation function\n",
    "    |Returns A - the activation of the layer\n",
    "    |       activation_cache - returns Z for backpropagation\n",
    "    '''\n",
    "    A = np.exp(Z)/sum(np.exp(Z))\n",
    "    activation_cache = {'Z':Z}\n",
    "    return A , activation_cache\n",
    "\n",
    "def relu(Z):\n",
    "    '''\n",
    "    |Z – the linear component of the activation function\n",
    "    |Returns A - the activation of the layer\n",
    "    |        activation_cache - returns Z for backpropagation\n",
    "    '''\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = {'Z':Z}\n",
    "    return A , activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, B, activation, dropout, training = False):\n",
    "    '''\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    |A_prev – activations of the previous layer\n",
    "    |W – the weights matrix of the current layer\n",
    "    |B – the bias vector of the current layer\n",
    "    |Activation – the activation function to be used (a string, either “softmax” or “relu”)\n",
    "    |dropout : use dropout\n",
    "    |training : is in training mode\n",
    "    Returns:\n",
    "    A – the activations of the current layer\n",
    "    cache – a joint dictionary containing both linear_cache and activation_cache\n",
    "    '''\n",
    "    activations = {'softmax':softmax, 'relu':relu}\n",
    "    Z, linear_cache = linear_forward(A_prev, W, B, dropout, training)\n",
    "    activation_cache = {}\n",
    "    A = 0\n",
    "    try:\n",
    "        A, activation_cache = activations[activation.lower()](Z)\n",
    "    except:\n",
    "        print('activation {} was not implemented'.format(activation))\n",
    "    # merge dicts to A W b & Z\n",
    "    cache = linear_cache\n",
    "    cache.update(activation_cache)\n",
    "    return A,  cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, use_batchnorm, dropout, training):\n",
    "    '''\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n",
    "    |X – the data, numpy array of shape (input size, number of examples)\n",
    "    |parameters – the initialized W and b parameters of each layer\n",
    "    |use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation \n",
    "    Returns:\n",
    "        AL – the last post-activation value\n",
    "        caches – a list of all the cache objects generated by the linear_forward function\n",
    "    '''\n",
    "    \n",
    "    weights = [(k, v) for (k, v) in parameters.items() if 'W' in k]\n",
    "    biases =[(k, v) for (k, v) in parameters.items() if 'b' in k]\n",
    "\n",
    "    caches = []\n",
    "    \n",
    "    # initalize input layer with inputs X\n",
    "    A = X\n",
    "    \n",
    "    # Apply relu on inner layers:\n",
    "    inner_layers_count = len(weights) - 1\n",
    "    activation_func = 'relu'\n",
    "    for layer in range(inner_layers_count):\n",
    "        # extract W and B for examined layer.\n",
    "        W = weights[layer][1:][0]\n",
    "        B = biases[layer][1:][0]\n",
    "        A_prev = A\n",
    "        # don't dropout any of the input neurons\n",
    "        dropout = dropout and layer != 0\n",
    "        #activate relu using W and B\n",
    "        A, cache = linear_activation_forward(A_prev, W, B, activation_func, dropout, training)\n",
    "        if use_batchnorm:\n",
    "            A = apply_batchnorm(A)\n",
    "        caches.append(cache)\n",
    "\n",
    "    #Apply softmax on last (output) layer & do not use batchnorm:\n",
    "    activation_func = 'softmax'\n",
    "    use_batchnorm = False\n",
    "    last_layer_index = len(weights) - 1\n",
    "    W = weights[last_layer_index][1:][0]\n",
    "    B = biases[last_layer_index][1:][0]\n",
    "    A_prev = A\n",
    "    AL, cache = linear_activation_forward(A_prev, W, B, activation_func, False, training)   \n",
    "    caches.append(cache)\n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    '''\n",
    "    Calculate cost function as categorical cross-entropy loss.\n",
    "    |AL – the last post-activation value\n",
    "    |Y  - the ground thruth\n",
    "    |returns cross entropy cost\n",
    "    '''\n",
    "    Y_hat = AL\n",
    "    inner_prod = Y * np.log(Y_hat)\n",
    "    #[(y1*log(y^1)]\n",
    "    inner_sum = np.sum(inner_prod, axis = 0)\n",
    "    outer_sum = np.mean(inner_sum)\n",
    "    cost =  -1 * outer_sum\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_batchnorm(A):\n",
    "    '''\n",
    "    performs batchnorm on the received activation values of a given layer.\n",
    "    |A - the activation values of a given layer\n",
    "    Returns:\n",
    "        NA - the normalized activation values, based on the formula learned in class\n",
    "    '''\n",
    "    epsilon = 1e-5\n",
    "    batch_mean = np.mean(A)\n",
    "    if np.isnan(batch_mean):\n",
    "        batch_mean = 0\n",
    "    batch_variance = np.var(A)\n",
    "    if np.isnan(batch_variance):\n",
    "\n",
    "        batch_variance = 0\n",
    "    numerator = A - batch_mean\n",
    "    denominator = np.sqrt(batch_variance + epsilon)\n",
    "    batch_normed = numerator / denominator\n",
    "    return batch_normed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKWARDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    '''\n",
    "    Implements the linear part of the backward propagation process for a single layer\n",
    "    |dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)\n",
    "    |cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current laye\n",
    "    Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1)\n",
    "        dW -- Gradient of the cost with respect to W (current layer l)\n",
    "        db -- Gradient of the cost with respect to b (current layer l)\n",
    "    '''\n",
    "    A_prev, W, b= cache['A'], cache['W'], cache['b']\n",
    "    layer_inputs_count = A_prev.shape[0]\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = float(1) / layer_inputs_count *  np.dot(dZ, A_prev.T)\n",
    "    db = float(1) /layer_inputs_count * np.sum(dZ, axis=1, keepdims=True)\n",
    "   \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    '''\n",
    "    Implements backward propagation for a ReLU unit\n",
    "    |dA – the post-activation gradient\n",
    "    |activation_cache – contains Z (stored during the forward propagation)\n",
    "    Returns:\n",
    "        dZ – gradient of the cost with respect to Z\n",
    "    '''\n",
    "    Z = activation_cache\n",
    "    dZ = np.copy(dA)\n",
    "    #  Reset dZ when z <= 0\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "        \n",
    "\n",
    "def softmax_backward(dA, activation_cache):\n",
    "    '''\n",
    "    Implements backward propagation for a softmax unit\n",
    "    |dA – the post-activation gradient\n",
    "    |activation_cache – contains Z (stored during the forward propagation)\n",
    "    Returns:\n",
    "        dZ – gradient of the cost with respect to Z\n",
    "    '''\n",
    "    Z = activation_cache \n",
    "    A,cache = softmax(Z) \n",
    "    dSig = np.multiply(A,(1-A)) \n",
    "    dZ = np.multiply(dA,dSig)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    '''\n",
    "    Implements the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    The function first computes dZ and then applies the linear_backward function.\n",
    "    |dA – post activation gradient of the current layer\n",
    "    |cache – contains both the linear cache and the activations cache\n",
    "    Returns:\n",
    "        dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1)\n",
    "        dW – Gradient of the cost with respect to W (current layer l)\n",
    "        db – Gradient of the cost with respect to b (current layer l)\n",
    "    '''\n",
    "    \n",
    "    # extract activation cache and linear cache from dict:\n",
    "    activation_cache = cache['Z']\n",
    "    linear_cache = {key: cache[key] for key in cache.keys() & {'A', 'W', 'b'}} \n",
    "    \n",
    "    activations_backwards = {'softmax':softmax_backward, 'relu':relu_backward}\n",
    "    \n",
    "    dZ = activations_backwards[activation.lower()](dA, activation_cache)\n",
    "    #dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "    return (dA_prev, dW, db)\n",
    "\n",
    "\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    '''\n",
    "    Implement the backward propagation process for the entire network.\n",
    "    |AL - the probabilities vector, the output of the forward propagation (L_model_forward)\n",
    "    |Y - the true labels vector (the \"ground truth\" - true classifications)\n",
    "    |Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache\n",
    "    Returns:\n",
    "        Grads - dictionary with the gradients.\n",
    "    '''\n",
    "    num_of_layers = len(caches)\n",
    "    last_layer_index = num_of_layers - 1\n",
    "    last_layer_cache = caches[last_layer_index]\n",
    "    grads = {}\n",
    "    # dZ = AL - Y\n",
    "    dZ = np.subtract(AL, Y)\n",
    "    epsilon = 1e-5\n",
    "    AL[AL == 1] -= epsilon\n",
    "    AL[AL == 0] += epsilon\n",
    "    #derivative of last layer\n",
    "    \n",
    "    dAL = -1 * (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    #backpropagation for the softmax function applied once as only the output layers uses it\n",
    "    \n",
    "    grads[\"dA{}\".format(str(num_of_layers))], grads[\"dW{}\".format(str(num_of_layers))],grads[\"db{}\".format(str(num_of_layers))] = linear_activation_backward(dAL, last_layer_cache, activation = \"softmax\")\n",
    "    # Apply RELU backwards activation on the rest of the layers:\n",
    "    \n",
    "\n",
    "    for l in reversed(range(1,num_of_layers)):\n",
    "        dA_prev = grads['dA{}'.format(str(l+1))] # the activation derivative of the l+1 layer (calculated above with softmax)\n",
    "        current_layer_cache = caches[l-1]\n",
    "        dA_prev, dW, db = linear_activation_backward(dA_prev, current_layer_cache, activation = \"relu\")\n",
    "        grads[\"dA{}\".format(str(l))] = dA_prev\n",
    "        grads[\"dW{}\".format(str(l))] = dW\n",
    "        grads[\"db{}\".format(str(l))] = db\n",
    "    return grads\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    Updates parameters using gradient descent\n",
    "    |parameters – a dictionary containing the DNN architecture’s parameters\n",
    "    |grads – a dictionary containing the gradients (generated by L_model_backward)\n",
    "    |learning_rate – the learning rate used to update the parameters (the “alpha”)\n",
    "    Returns:\n",
    "        parameters – the updated values of the parameters object provided as input\n",
    "    '''\n",
    "    weights =  {key: parameters[key] for key in parameters.keys() if 'W' in key}\n",
    "    biases = {key: parameters[key] for key in parameters.keys() if 'b' in key} \n",
    "    num_of_layers = len(weights.keys())\n",
    "    for l in range(num_of_layers):\n",
    "        #update Wl+1 = WL - alpha*dWL+1\n",
    "         parameters[\"W{}\".format(str(l+1))] -= grads[\"dW{}\".format(str(l+1))] * learning_rate\n",
    "        #update  bl+1 = bL - alpha*dbL+1\n",
    "         parameters[\"b{}\".format(str(l+1))] -= grads[\"db{}\".format(str(l+1))] * learning_rate\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT Neural Network with L layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, Y , num_of_classes, test_size = 0.2):\n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size= test_size)\n",
    "    X_train = np.swapaxes(X_train.reshape(-1, 28*28), 0, 1)\n",
    "    X_validation = np.swapaxes(X_validation.reshape(-1, 28*28), 0, 1)\n",
    "    X_train = X_train / 255\n",
    "    X_validation = X_validation / 255\n",
    "    Y_train = np.swapaxes(np_utils.to_categorical(Y_train, num_of_classes), 0, 1)\n",
    "    Y_validation = np.swapaxes(np_utils.to_categorical(Y_validation, num_of_classes), 0, 1)\n",
    "    return X_train, X_validation , Y_train, Y_validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate, num_epochs, batch_size, use_batchnorm, dropout, improvement_epsilon, verbose = False):\n",
    "    '''\n",
    "    Implements a L-layer neural network\n",
    "    Last layer apply softmax\n",
    "    Rest of the layers apply Relu\n",
    "    Network Output is equal to the number of labels within the data\n",
    "    Logic:\n",
    "    initialize -> L_model_forward -> compute_cost -> L_model_backward -> update parameters\n",
    "    \n",
    "    |X– the input data (GRAYSCALE), a numpy array of shape (height*width , number_of_examples)\n",
    "    |Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)\n",
    "    |Layer_dims – a list containing the dimensions of each layer\n",
    "    |batch_size – the number of examples in a single training batch.\n",
    "    Returns:\n",
    "        parameters – the parameters learnt by the system during the training \n",
    "        train_costs & validation_costs – the values of the cost function (one value per 100 iterations)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Parameters initialization.\n",
    "    epsilon = sys.float_info.epsilon\n",
    "    old_cost = math.inf\n",
    "    min_validation_cost = 1000000\n",
    "    min_validation_cost_age = 0\n",
    "    num_of_classes = 10\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    train_costs , validation_costs = [] , []\n",
    "    #inialize cost with high val \n",
    "\n",
    "   \n",
    "    X_train, X_validation , Y_train, Y_validation = create_dataset(X, Y , num_of_classes, 0.2)\n",
    "   \n",
    "    # Start Gradient descent: (the only loop)\n",
    "    num_training_samples = X_train.shape[1]\n",
    "    shuffle_train = [i for i in range(num_training_samples)]\n",
    "    stop = False\n",
    "    iterations_completed = 0\n",
    "    epochs_completed = 0\n",
    "    \n",
    "    for j in range(num_epochs):\n",
    "        if stop:\n",
    "            break\n",
    "        \n",
    "        sum_validations_costs = 0\n",
    "        \n",
    "        # shuffle indices (training samples order) before every epoch\n",
    "        np.random.shuffle(shuffle_train)\n",
    "        \n",
    "        # run through the epoch batch by batch\n",
    "        num_batches = math.ceil(num_training_samples / batch_size)\n",
    "        print('num of samples {}'.format(num_training_samples))\n",
    "        print('num of batched : {}'.format(num_batches))\n",
    "        for i in range(num_batches):\n",
    "            # batch size is the standard size, except for the last batch which is however many samples are left (less than or equal to a normal batch)\n",
    "            this_batch_size = batch_size if i < (num_batches-1) else (num_training_samples - num_batches * i)\n",
    "            batch_indices = shuffle_train[i * batch_size : (i * batch_size) + this_batch_size]\n",
    "\n",
    "            # now do a single iteration\n",
    "            \n",
    "            X_batch = X_train[:, batch_indices]\n",
    "            Y_batch = Y_train[:, batch_indices]\n",
    "\n",
    "            # L_model_forward\n",
    "            AL, caches = L_model_forward(X_batch, parameters, use_batchnorm = use_batchnorm, dropout = dropout, training = True)\n",
    "            # Compute_cost\n",
    "            train_cost = compute_cost(AL, Y_batch)\n",
    "            # L_model_backward\n",
    "            grads = L_model_backward(AL, Y_batch, caches)\n",
    "            # Update parameters\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "\n",
    "            # VALIDATION ( training = False)\n",
    "            validation_AL, validation_Caches = L_model_forward(X_validation, parameters,use_batchnorm = use_batchnorm, dropout = dropout, training = False)\n",
    "            validation_cost = compute_cost(validation_AL, Y_validation)\n",
    "            # append costs\n",
    "            \n",
    "            if (iterations_completed % 100 == 0):\n",
    "                if verbose:\n",
    "                    print('finished iteration {} - train score - {} , validation score - {} '.format(i, train_costs, validation_costs), end=': ')\n",
    "                else:\n",
    "                    print('finished iteration {} ,finished {} epochs'.format(iterations_completed, j), end=': ')\n",
    "                print('train cost is {:.4f} ||| validation cost is {:.4f}'.format(train_cost, validation_cost))\n",
    "                train_costs.append(train_cost + epsilon)\n",
    "                validation_costs.append(validation_cost + epsilon)\n",
    "            \n",
    "            iterations_completed += 1\n",
    "            sum_validations_costs += validation_cost\n",
    "            \n",
    "            #stop if costs not improves for 100 iterations in a row\n",
    "            if min_validation_cost - validation_cost > improvement_epsilon:\n",
    "                min_validation_cost = validation_cost\n",
    "                min_validation_cost_age = 1\n",
    "            elif min_validation_cost_age < 100:\n",
    "                min_validation_cost_age += 1\n",
    "            else:\n",
    "                print(\"cost score not improving for 100 iterations.\")\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "            old_cost = validation_cost\n",
    "            \n",
    "                \n",
    "        epochs_completed += 1\n",
    "        print('Epoch {} completed. Average validation cost was {:.4f}'.format(epochs_completed, sum_validations_costs/num_batches))\n",
    "        sum_validations_costs = 0\n",
    "    \n",
    "    \n",
    "    print('train_costs {}:'.format(train_costs))\n",
    "    print('validation_costs {}:'.format(validation_costs))\n",
    "\n",
    "    train_accuracy = predict(X_train, Y_train, parameters, use_batchnorm, dropout)\n",
    "    val_accuracy = predict(X_validation, Y_validation, parameters, use_batchnorm, dropout)\n",
    "    return parameters, train_costs, validation_costs, train_accuracy, val_accuracy, epochs_completed, iterations_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strongest_index(AL, index):\n",
    "    '''\n",
    "    return the index of the strongest label (the label which receives the hughest confidence score)\n",
    "    '''\n",
    "    return np.argmax(AL[:, index], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "def predict(X, Y, parameters, use_batchnorm, dropout):\n",
    "    '''\n",
    "    receives an input data and the true labels and calculates the accuracy of the trained neural network on the data\n",
    "    |X – the input data, a numpy array of shape (height*width, number_of_examples)\n",
    "    |Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)\n",
    "    |Parameters – a dictionary containing the DNN architecture’s parameters\n",
    "    Returns:\n",
    "        accuracy – the accuracy measure of the neural net on the provided data\n",
    "    '''\n",
    "    # apply model_forward over X\n",
    "    samples = Y.shape[1]\n",
    "    AL, caches = L_model_forward(X, parameters, use_batchnorm=use_batchnorm, dropout= dropout, training = False)\n",
    "    Y_pred = np.zeros(Y.shape)\n",
    "    for i in range(samples):\n",
    "        label = get_strongest_index(AL, index = i)\n",
    "        Y_pred[label, i] = 1\n",
    "    Y_pred = np.swapaxes(Y_pred, 0, 1)\n",
    "    Y = np.swapaxes(Y, 0, 1)\n",
    "    return accuracy_score(Y_pred, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(output_path, fields_name,class_num, epochs, batch_size, arch ,learning_rate, verbose,):\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    IMAGE_SIZE = 28 * 28\n",
    "    x_test = np.swapaxes(x_test.reshape(-1, IMAGE_SIZE), 0, 1)\n",
    "    x_test = x_test / 255\n",
    "    y_test = np.swapaxes(np_utils.to_categorical(y_test, class_num), 0, 1)\n",
    "    options = [True, False]\n",
    "    with open(output_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fields_name)\n",
    "        writer.writeheader()\n",
    "    \n",
    "    for dropout in options:\n",
    "        for bachnorm in options:\n",
    "            print(' -------- WORKING ON DROP {} BACHNORM {} --------'.format(dropout , bachnorm))\n",
    "            params, train_cost, validation_cost, train_accuracy, val_accuracy, epochs_completed, iterations_completed = L_layer_model(x_train, y_train, arch, learning_rate, epochs, batch_size, bachnorm, dropout, improvement_epsilon, verbose)\n",
    "            test_accuracy = predict(x_test, y_test, params, bachnorm, dropout)\n",
    "            with open(output_path, mode='a', newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=fields_name)\n",
    "                writer.writerow({'Learning Rate': str(learning_rate), 'Bachnrom': str(bachnorm), 'dropout': str(dropout),'test accuracy': str(test_accuracy),\n",
    "                                 'validation accuracy': str(val_accuracy), 'train accuracy': str(train_accuracy), 'epochs_completed': str(epochs_completed), 'iterations_completed': str(iterations_completed)\n",
    "                                 })\n",
    "\n",
    "                pd.DataFrame.from_dict(train_cost).to_csv('train_costs-{}-{}.csv'.format(dropout,bachnorm))\n",
    "                pd.DataFrame.from_dict(validation_cost).to_csv('validation_costs-{}-{}.csv'.format(dropout,bachnorm))\n",
    "\n",
    "arch = [28*28, 20, 7, 5, 10]\n",
    "classes = 10\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "learning_rate = 0.009\n",
    "improvement_epsilon = 0.000001\n",
    "fields_name = ['Learning Rate', 'Bachnrom', 'dropout', 'test accuracy', 'validation accuracy', 'train accuracy', 'epochs_completed', 'iterations_completed']\n",
    "main('final_res.csv', fields_name, classes, epochs, batch_size, arch,learning_rate, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
